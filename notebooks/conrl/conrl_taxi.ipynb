{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.conrl import ConRL\n",
    "from src.qlearning import QLearningAgent\n",
    "from src.utils import *\n",
    "from src.plotting import *\n",
    "\n",
    "import itertools\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "import time\n",
    "import sys\n",
    "import copy\n",
    "from collections import namedtuple\n",
    "import wandb \n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.collections as mc\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-paper')\n",
    "np.set_printoptions(precision=3, linewidth=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "print('State Space ', env.observation_space) \n",
    "print('State Space sample', env.observation_space.sample())\n",
    "print('Action Space ', env.action_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = (env.observation_space.n, )\n",
    "\n",
    "num_episodes = 500\n",
    "max_step = 1000\n",
    "env._max_episode_steps = max_step\n",
    "\n",
    "q_params = {\n",
    "    \"gamma\": 0.6,\n",
    "    \"alpha\": 0.1,\n",
    "    \"alpha_decay_rate\": 0,\n",
    "    \"min_alpha\": 0.1,\n",
    "    \"epsilon\": 0.9,\n",
    "    \"epsilon_decay_rate\": 0,\n",
    "    \"min_epsilon\": 0.01\n",
    "}\n",
    "\n",
    "q_params[\"epsilon_decay_rate\"] = (q_params[\"epsilon\"] - q_params[\"min_epsilon\"])/(num_episodes//2)\n",
    "q_params[\"alpha_decay_rate\"] = (q_params[\"alpha\"] - q_params[\"min_alpha\"])/(num_episodes//2)\n",
    "\n",
    "mlgng_params = {\n",
    "    \"ndim\": 4, \n",
    "    \"e_w\":0.05, \n",
    "    \"e_n\":0.005, \n",
    "    \"l\":10, \n",
    "    \"a\":0.5, \n",
    "    \"b\":0.95,\n",
    "    \"k\":1000.0, \n",
    "    \"max_nodes\": 10, \n",
    "    \"max_age\": 200\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats_q =   {\n",
    "            \"step\":  np.zeros(num_episodes),\n",
    "            \"cumulative_reward\":  np.zeros(num_episodes),\n",
    "            \"q_tables\": np.zeros(shape = (num_episodes, ) + state_size + (env.action_space.n, )),\n",
    "            \"best_actions\": []\n",
    "            }\n",
    "\n",
    "q_agent = QLearningAgent(action_size=env.action_space.n, state_size=state_size, **q_params)\n",
    "\n",
    "q_agent.train(env, num_episodes, stats_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.plotting import *\n",
    "\n",
    "act_dict = {\n",
    "    0: \"Push left\",\n",
    "    1: \"No push\",\n",
    "    2: \"Push right\"\n",
    "}\n",
    "\n",
    "act_symbol = {\n",
    "    0:  \"o\",\n",
    "    1: \"^\",\n",
    "    2: \"s\",\n",
    "}\n",
    "act_color = cm.Dark2(np.linspace(0.1, 1, 3, endpoint=False))\n",
    "\n",
    "act_symbol_plotly = {0:0, 1:5, 2:1}\n",
    "act_color_plotly = act_color.copy()\n",
    "act_color_plotly[:, 0:-1]=act_color_plotly[:, 0:-1]*255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {\n",
    "        \"step\":  np.zeros(num_episodes),\n",
    "        \"cumulative_reward\":  np.zeros(num_episodes),\n",
    "        \"selector\":    np.zeros(num_episodes),\n",
    "        \"global_error\":     np.zeros((num_episodes, env.action_space.n)),\n",
    "        \"mlgng_nodes\":      [],\n",
    "        \"best_actions\":     [],\n",
    "        \"nodes\": np.zeros((num_episodes, env.action_space.n)),\n",
    "        \"rate\": np.zeros(num_episodes),\n",
    "        \"max_avg_reward\": np.zeros(num_episodes),\n",
    "}\n",
    "\n",
    "conrl = ConRL(action_size=env.action_space.n, state_size=state_size, update_threshold=10)\n",
    "support = QLearningAgent(action_size=env.action_space.n, state_size=state_size, **q_params)\n",
    "conrl.init_support(support)\n",
    "conrl.init_mlgng(**mlgng_params)\n",
    "\n",
    "conrl.train(env=env, stats=stats, num_episodes=500, print_freq=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = [\"step\", \"cumulative_reward\"]\n",
    "plot_stats_comparison({\n",
    "    \"Vanilla Q\": {key: value for key, value in stats_q.items() if key in to_plot}, \n",
    "    \"Con-RL\": {key: value for key, value in stats.items() if key in to_plot}\n",
    "},\n",
    "title=\"Con-RL v. Vanilla Q\",\n",
    "rolling_window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nodes_changes(stats[\"mlgng_nodes\"], \n",
    "                rewards=stats[\"cumulative_reward\"],\n",
    "                action_names=act_dict, \n",
    "                symbols=act_symbol_plotly, \n",
    "                colors=act_color_plotly,\n",
    "                frequency=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
